{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Detecci贸n y caracterizaci贸n de caras",
   "id": "14db5d0926ec87c2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Entrenamiento para detecci贸n de attributos",
   "id": "36c1b5ae4e603ba8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T20:43:37.612718Z",
     "start_time": "2025-11-15T20:27:03.199923Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from pygments.styles.dracula import background\n",
    "from sympy import horner\n",
    "from sympy.physics.vector import get_motion_params\n",
    "from torchvision import models, transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "EPOCHS = 35\n",
    "DATASET_BASE_DIRECTORY = \"../emotion-recognition-dataset/\"\n",
    "BATCH_SIZE = 32\n",
    "NUM_CLASSES = 8\n",
    "last_accuracy = 0.5844\n",
    "\n",
    "# --- Data transforms ---\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "# Load your dataset\n",
    "train_dataset = datasets.ImageFolder(DATASET_BASE_DIRECTORY + \"train\", transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "val_dataset = datasets.ImageFolder(DATASET_BASE_DIRECTORY + \"val\", transform=transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# --- Load EfficientNet ---\n",
    "weights = models.EfficientNet_B2_Weights.IMAGENET1K_V1\n",
    "emotion_model = models.efficientnet_b2(weights=weights)\n",
    "# Replace classifier for 8 emotion classes\n",
    "num_features = emotion_model.classifier[1].in_features\n",
    "emotion_model.classifier[1] = nn.Linear(num_features, NUM_CLASSES)\n",
    "\n",
    "\n",
    "for param in emotion_model.features[:5].parameters():  # adjust slice as needed\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Move to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "emotion_model = emotion_model.to(device)\n",
    "\n",
    "# --- Loss & optimizer ---\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(emotion_model.parameters(), lr=1e-4)\n",
    "\n",
    "# --- Training loop (simplified) ---\n",
    "for epoch in range(EPOCHS):\n",
    "    emotion_model.train()\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = emotion_model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch} done\")\n",
    "\n",
    "\n",
    "# --- Validation loop ---\n",
    "emotion_model.eval()  # set model to evaluation mode\n",
    "running_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():  # disable gradient computation\n",
    "    for imgs, labels in val_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        outputs = emotion_model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "\n",
    "        # Compute number of correct predictions\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "avg_loss = running_loss / total\n",
    "accuracy = correct / total\n",
    "\n",
    "print(f\"Validation Loss: {avg_loss:.4f}, Accuracy: {accuracy*100:.2f}%\")\n",
    "\n",
    "if accuracy > last_accuracy:\n",
    "    torch.save(emotion_model.state_dict(), \"emotion-efficientnet-weights.pth\")\n"
   ],
   "id": "e300d61a4dd2cb4a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 done\n",
      "Epoch 1 done\n",
      "Epoch 2 done\n",
      "Epoch 3 done\n",
      "Epoch 4 done\n",
      "Epoch 5 done\n",
      "Epoch 6 done\n",
      "Epoch 7 done\n",
      "Epoch 8 done\n",
      "Epoch 9 done\n",
      "Epoch 10 done\n",
      "Epoch 11 done\n",
      "Epoch 12 done\n",
      "Epoch 13 done\n",
      "Epoch 14 done\n",
      "Epoch 15 done\n",
      "Epoch 16 done\n",
      "Epoch 17 done\n",
      "Epoch 18 done\n",
      "Epoch 19 done\n",
      "Epoch 20 done\n",
      "Epoch 21 done\n",
      "Epoch 22 done\n",
      "Epoch 23 done\n",
      "Epoch 24 done\n",
      "Epoch 25 done\n",
      "Epoch 26 done\n",
      "Epoch 27 done\n",
      "Epoch 28 done\n",
      "Epoch 29 done\n",
      "Epoch 30 done\n",
      "Epoch 31 done\n",
      "Epoch 32 done\n",
      "Epoch 33 done\n",
      "Epoch 34 done\n",
      "Validation Loss: 1.9946, Accuracy: 60.50%\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Filtro de emociones",
   "id": "64f8e68708060d76"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-16T15:13:48.738698Z",
     "start_time": "2025-11-16T15:13:35.630198Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import filters\n",
    "from deepface import DeepFace\n",
    "\n",
    "# happy, angry, fear, neutral, sad, disgust, surprise\n",
    "filters_dict = {\n",
    "    \"happy\": filters.HappyFilter(\"resources/images/aureola.png\"),\n",
    "    \"angry\": filters.AngryFilter(\"resources/images/devil-horns.png\"),\n",
    "    \"sad\": filters.SadFilter(\"resources/images/tears.png\"),\n",
    "    \"fear\": filters.FearFilter(\"resources/images/cold-sweat.png\"),\n",
    "    \"surprise\": filters.SurpriseFilter(\"resources/images/surprise.png\", \"resources/images/surprised_eye.png\"),\n",
    "}\n",
    "\n",
    "def predict_emotion(img, face_data):\n",
    "    x, y, w, h = face_data['facial_area']['x'], face_data['facial_area']['y'], face_data['facial_area']['w'], face_data['facial_area']['h']\n",
    "    cropped_face_rgb = cv2.cvtColor(img[y:y + h, x:x + w], cv2.COLOR_BGR2RGB)\n",
    "    objs = DeepFace.analyze(cropped_face_rgb, actions = ['emotion'], enforce_detection=False)\n",
    "    dominant_emotion = objs[0]['dominant_emotion']\n",
    "    cv2.putText(img, f\"Emotion: {dominant_emotion}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "    return dominant_emotion\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "ret, frame = cap.read()\n",
    "key = 0\n",
    "\n",
    "while ret and key != 27:\n",
    "    try:\n",
    "        for face in DeepFace.extract_faces(frame, detector_backend=\"yolov8\", enforce_detection=False):\n",
    "            filters_dict.get(predict_emotion(frame, face), filters.NoFilter()).apply(frame, face)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"error: {e}\")\n",
    "\n",
    "    cv2.imshow(\"Video\", frame)\n",
    "    ret, frame = cap.read()\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "# Liberar la captura y cerrar ventanas\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "id": "9cc6a544fd95b67a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-16 15:13:36.004541: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-16 15:13:36.046603: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-11-16 15:13:37.038042: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-16 15:13:41.582490: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1763306021.583458   36671 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1991 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2050, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2025-11-16 15:13:41.713118: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:473] Loaded cuDNN version 91002\n",
      "Warning: Ignoring XDG_SESSION_TYPE=wayland on Gnome. Use QT_QPA_PLATFORM=wayland to run on Wayland anyway.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Filtro libre",
   "id": "5c06055d916b05c4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Funci贸n para guardar un gif en memoria",
   "id": "3700632e57494805"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T16:09:55.918573Z",
     "start_time": "2025-11-17T16:09:55.916219Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import filters\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import imageio\n",
    "\n",
    "def read_gif(path, width, height):\n",
    "    gif = imageio.mimread(path)\n",
    "    frames = [cv2.resize(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR), (width, height)) for frame in gif]\n",
    "    return frames"
   ],
   "id": "175f05730f4d5de7",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T16:17:55.684116Z",
     "start_time": "2025-11-17T16:17:44.966572Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mp_face_mesh = mp.solutions.face_mesh.FaceMesh(static_image_mode=True)\n",
    "\n",
    "cap = cv2.VideoCapture(\"resources/test/face_1.mp4\")\n",
    "ret, frame = cap.read()\n",
    "key = 0\n",
    "galaxy_filter = filters.GalaxyFilter()\n",
    "debug_filter = filters.MediapipeDebugFilter()\n",
    "\n",
    "frame_number = 0\n",
    "w,h,_ = frame.shape\n",
    "\n",
    "FRAME_WIDTH = 520\n",
    "FRAME_HEIGHT = int(FRAME_WIDTH * h/w)\n",
    "\n",
    "background_gif_frames = read_gif(\"resources/images/matrix_background.gif\", FRAME_WIDTH, FRAME_HEIGHT)\n",
    "while ret and key != 27:\n",
    "    try:\n",
    "        # Just for this video\n",
    "        frame = cv2.rotate(frame, cv2.ROTATE_90_CLOCKWISE)\n",
    "\n",
    "        frame = cv2.resize(frame, (FRAME_WIDTH, FRAME_HEIGHT))\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame = background_gif_frames[frame_number].copy()\n",
    "\n",
    "        for face in mp_face_mesh.process(frame_rgb).multi_face_landmarks:\n",
    "            debug_filter.apply(frame, face)\n",
    "\n",
    "        # Just for this video\n",
    "        time.sleep(0.02)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"error: {e}\")\n",
    "\n",
    "    cv2.imshow(\"Video\", frame)\n",
    "    ret, frame = cap.read()\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    frame_number = (frame_number + 1) % len(background_gif_frames)\n",
    "\n",
    "\n",
    "# Liberar la captura y cerrar ventanas\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ],
   "id": "c7de0dea193c0e13",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1763396264.978581   19180 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1763396264.979922   20025 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 25.1.9), renderer: Mesa Intel(R) UHD Graphics (TGL GT1)\n",
      "W0000 00:00:1763396264.982496   20021 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1763396264.988454   20020 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "execution_count": 16
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
