{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Detección y caracterización de caras",
   "id": "14db5d0926ec87c2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Entrenamiento para detección de attributos",
   "id": "36c1b5ae4e603ba8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T20:43:37.612718Z",
     "start_time": "2025-11-15T20:27:03.199923Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sympy import horner\n",
    "from sympy.physics.vector import get_motion_params\n",
    "from torchvision import models, transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "EPOCHS = 35\n",
    "DATASET_BASE_DIRECTORY = \"../emotion-recognition-dataset/\"\n",
    "BATCH_SIZE = 32\n",
    "NUM_CLASSES = 8\n",
    "last_accuracy = 0.5844\n",
    "\n",
    "# --- Data transforms ---\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "# Load your dataset\n",
    "train_dataset = datasets.ImageFolder(DATASET_BASE_DIRECTORY + \"train\", transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "val_dataset = datasets.ImageFolder(DATASET_BASE_DIRECTORY + \"val\", transform=transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# --- Load EfficientNet ---\n",
    "weights = models.EfficientNet_B2_Weights.IMAGENET1K_V1\n",
    "emotion_model = models.efficientnet_b2(weights=weights)\n",
    "# Replace classifier for 8 emotion classes\n",
    "num_features = emotion_model.classifier[1].in_features\n",
    "emotion_model.classifier[1] = nn.Linear(num_features, NUM_CLASSES)\n",
    "\n",
    "\n",
    "for param in emotion_model.features[:5].parameters():  # adjust slice as needed\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Move to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "emotion_model = emotion_model.to(device)\n",
    "\n",
    "# --- Loss & optimizer ---\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(emotion_model.parameters(), lr=1e-4)\n",
    "\n",
    "# --- Training loop (simplified) ---\n",
    "for epoch in range(EPOCHS):\n",
    "    emotion_model.train()\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = emotion_model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch} done\")\n",
    "\n",
    "\n",
    "# --- Validation loop ---\n",
    "emotion_model.eval()  # set model to evaluation mode\n",
    "running_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():  # disable gradient computation\n",
    "    for imgs, labels in val_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        outputs = emotion_model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "\n",
    "        # Compute number of correct predictions\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "avg_loss = running_loss / total\n",
    "accuracy = correct / total\n",
    "\n",
    "print(f\"Validation Loss: {avg_loss:.4f}, Accuracy: {accuracy*100:.2f}%\")\n",
    "\n",
    "if accuracy > last_accuracy:\n",
    "    torch.save(emotion_model.state_dict(), \"emotion-efficientnet-weights.pth\")\n"
   ],
   "id": "e300d61a4dd2cb4a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 done\n",
      "Epoch 1 done\n",
      "Epoch 2 done\n",
      "Epoch 3 done\n",
      "Epoch 4 done\n",
      "Epoch 5 done\n",
      "Epoch 6 done\n",
      "Epoch 7 done\n",
      "Epoch 8 done\n",
      "Epoch 9 done\n",
      "Epoch 10 done\n",
      "Epoch 11 done\n",
      "Epoch 12 done\n",
      "Epoch 13 done\n",
      "Epoch 14 done\n",
      "Epoch 15 done\n",
      "Epoch 16 done\n",
      "Epoch 17 done\n",
      "Epoch 18 done\n",
      "Epoch 19 done\n",
      "Epoch 20 done\n",
      "Epoch 21 done\n",
      "Epoch 22 done\n",
      "Epoch 23 done\n",
      "Epoch 24 done\n",
      "Epoch 25 done\n",
      "Epoch 26 done\n",
      "Epoch 27 done\n",
      "Epoch 28 done\n",
      "Epoch 29 done\n",
      "Epoch 30 done\n",
      "Epoch 31 done\n",
      "Epoch 32 done\n",
      "Epoch 33 done\n",
      "Epoch 34 done\n",
      "Validation Loss: 1.9946, Accuracy: 60.50%\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Filtro de emociones",
   "id": "64f8e68708060d76"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T22:17:44.455178Z",
     "start_time": "2025-11-15T22:17:31.049992Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import cv2\n",
    "from deepface import DeepFace\n",
    "\n",
    "\n",
    "class Filter(ABC):\n",
    "    @abstractmethod\n",
    "    def apply(self, img, face_x, face_y, face_w, face_h):\n",
    "        pass\n",
    "\n",
    "class HappyFilter(Filter):\n",
    "    def __init__(self, halo_image_path):\n",
    "        self.halo_overlay = cv2.imread(halo_image_path, cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "    def apply(self, img, face_x, face_y, face_w, face_h):\n",
    "        offset_y = int(0.5 * face_h)  # adjust 0.5 to move more or less\n",
    "        halo_width = int(face_w * 1.5)\n",
    "        halo_height = int(face_h * 0.3)  # or resize proportionally\n",
    "\n",
    "        halo_x = max(int(face_x - face_w / 4), 0)\n",
    "        halo_y = max(face_y - offset_y, 0)  # make sure we don’t go negative\n",
    "        halo_resized = cv2.resize(self.halo_overlay, (halo_width, halo_height))\n",
    "        halo_rgb = halo_resized[:, :, :3]\n",
    "        alpha_mask = halo_resized[:, :, 3] / 255.0\n",
    "\n",
    "        roi = img[halo_y:halo_y + halo_height, halo_x:halo_x + halo_width]\n",
    "\n",
    "        for c in range(3):\n",
    "            roi[:, :, c] = (alpha_mask * halo_rgb[:, :, c] + (1 - alpha_mask) * roi[:, :, c])\n",
    "\n",
    "        img[halo_y:halo_y + halo_height, halo_x:halo_x + halo_width] = roi\n",
    "\n",
    "class AngryFilter(Filter):\n",
    "    def __init__(self, demon_img_path):\n",
    "        self.demon_overlay = cv2.imread(demon_img_path, cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "    def apply(self, img, face_x, face_y, face_w, face_h):\n",
    "        offset_y = int(face_h*0.2)  # adjust 0.5 to move more or less\n",
    "        horns_width = face_w\n",
    "        horns_height = int(face_h * 0.4)  # or resize proportionally\n",
    "\n",
    "        horns_x = face_x\n",
    "        horns_y = max(face_y - offset_y, 0)  # make sure we don’t go negative\n",
    "        horns_resized = cv2.resize(self.demon_overlay, (horns_width, horns_height))\n",
    "        horns_rgb = horns_resized[:, :, :3]\n",
    "        alpha_mask = horns_resized[:, :, 3] / 255.0\n",
    "\n",
    "        roi = img[horns_y:horns_y + horns_height, horns_x:horns_x + horns_width]\n",
    "\n",
    "        for c in range(3):\n",
    "            roi[:, :, c] = (alpha_mask * horns_rgb[:, :, c] + (1 - alpha_mask) * roi[:, :, c])\n",
    "\n",
    "        img[horns_y:horns_y + horns_height, horns_x:horns_x + horns_width] = roi\n",
    "\n",
    "class NoFilter(Filter):\n",
    "    def apply(self, img, face_x, face_y, face_w, face_h):\n",
    "        pass\n",
    "\n",
    "# happy, angry, fear, neutral, sad, disgust, surprise\n",
    "filters = {\n",
    "    \"happy\": HappyFilter(\"resources/images/aureola.png\"),\n",
    "    \"angry\": AngryFilter(\"resources/images/devil-horns.png\"),\n",
    "}\n",
    "\n",
    "def predict_emotion(cropped_face):\n",
    "    cropped_face_rgb = cv2.cvtColor(cropped_face, cv2.COLOR_BGR2RGB)\n",
    "    objs = DeepFace.analyze(cropped_face_rgb, actions = ['emotion'], enforce_detection=False)\n",
    "    return objs[0][\"dominant_emotion\"]\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "ret, frame = cap.read()\n",
    "key = 0\n",
    "\n",
    "while ret and key != 27:\n",
    "    try:\n",
    "        for face in DeepFace.extract_faces(frame, detector_backend=\"yolov8\", enforce_detection=False):\n",
    "            x, y, w, h = face['facial_area']['x'], face['facial_area']['y'], face['facial_area']['w'], face['facial_area']['h']\n",
    "            filters.get(\n",
    "                predict_emotion(frame[y:y + h, x:x + w]),\n",
    "                NoFilter()\n",
    "            ).apply(frame, x, y, w, h)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "\n",
    "    cv2.imshow(\"Video\", frame)\n",
    "    ret, frame = cap.read()\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "# Liberar la captura y cerrar ventanas\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "id": "9cc6a544fd95b67a",
   "outputs": [],
   "execution_count": 45
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
