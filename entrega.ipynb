{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Detecci贸n y caracterizaci贸n de caras",
   "id": "14db5d0926ec87c2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Entrenamiento para detecci贸n de attributos",
   "id": "36c1b5ae4e603ba8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T20:43:37.612718Z",
     "start_time": "2025-11-15T20:27:03.199923Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from pygments.styles.dracula import background\n",
    "from sympy import horner\n",
    "from sympy.physics.vector import get_motion_params\n",
    "from torchvision import models, transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "EPOCHS = 35\n",
    "DATASET_BASE_DIRECTORY = \"../emotion-recognition-dataset/\"\n",
    "BATCH_SIZE = 32\n",
    "NUM_CLASSES = 8\n",
    "last_accuracy = 0.5844\n",
    "\n",
    "# --- Data transforms ---\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "# Load your dataset\n",
    "train_dataset = datasets.ImageFolder(DATASET_BASE_DIRECTORY + \"train\", transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "val_dataset = datasets.ImageFolder(DATASET_BASE_DIRECTORY + \"val\", transform=transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# --- Load EfficientNet ---\n",
    "weights = models.EfficientNet_B2_Weights.IMAGENET1K_V1\n",
    "emotion_model = models.efficientnet_b2(weights=weights)\n",
    "# Replace classifier for 8 emotion classes\n",
    "num_features = emotion_model.classifier[1].in_features\n",
    "emotion_model.classifier[1] = nn.Linear(num_features, NUM_CLASSES)\n",
    "\n",
    "\n",
    "for param in emotion_model.features[:5].parameters():  # adjust slice as needed\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Move to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "emotion_model = emotion_model.to(device)\n",
    "\n",
    "# --- Loss & optimizer ---\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(emotion_model.parameters(), lr=1e-4)\n",
    "\n",
    "# --- Training loop (simplified) ---\n",
    "for epoch in range(EPOCHS):\n",
    "    emotion_model.train()\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = emotion_model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch} done\")\n",
    "\n",
    "\n",
    "# --- Validation loop ---\n",
    "emotion_model.eval()  # set model to evaluation mode\n",
    "running_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():  # disable gradient computation\n",
    "    for imgs, labels in val_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        outputs = emotion_model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "\n",
    "        # Compute number of correct predictions\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "avg_loss = running_loss / total\n",
    "accuracy = correct / total\n",
    "\n",
    "print(f\"Validation Loss: {avg_loss:.4f}, Accuracy: {accuracy*100:.2f}%\")\n",
    "\n",
    "if accuracy > last_accuracy:\n",
    "    torch.save(emotion_model.state_dict(), \"emotion-efficientnet-weights.pth\")\n"
   ],
   "id": "e300d61a4dd2cb4a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 done\n",
      "Epoch 1 done\n",
      "Epoch 2 done\n",
      "Epoch 3 done\n",
      "Epoch 4 done\n",
      "Epoch 5 done\n",
      "Epoch 6 done\n",
      "Epoch 7 done\n",
      "Epoch 8 done\n",
      "Epoch 9 done\n",
      "Epoch 10 done\n",
      "Epoch 11 done\n",
      "Epoch 12 done\n",
      "Epoch 13 done\n",
      "Epoch 14 done\n",
      "Epoch 15 done\n",
      "Epoch 16 done\n",
      "Epoch 17 done\n",
      "Epoch 18 done\n",
      "Epoch 19 done\n",
      "Epoch 20 done\n",
      "Epoch 21 done\n",
      "Epoch 22 done\n",
      "Epoch 23 done\n",
      "Epoch 24 done\n",
      "Epoch 25 done\n",
      "Epoch 26 done\n",
      "Epoch 27 done\n",
      "Epoch 28 done\n",
      "Epoch 29 done\n",
      "Epoch 30 done\n",
      "Epoch 31 done\n",
      "Epoch 32 done\n",
      "Epoch 33 done\n",
      "Epoch 34 done\n",
      "Validation Loss: 1.9946, Accuracy: 60.50%\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Filtro de emociones",
   "id": "64f8e68708060d76"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T10:43:27.378355Z",
     "start_time": "2025-11-19T10:42:26.500015Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import filters\n",
    "from deepface import DeepFace\n",
    "\n",
    "# happy, angry, fear, neutral, sad, disgust, surprise\n",
    "filters_dict = {\n",
    "    \"happy\": filters.HappyFilter(\"resources/images/aureola.png\"),\n",
    "    \"angry\": filters.AngryFilter(\"resources/images/devil-horns.png\"),\n",
    "    \"sad\": filters.SadFilter(\"resources/images/tears.png\"),\n",
    "    \"fear\": filters.FearFilter(\"resources/images/cold-sweat.png\"),\n",
    "    \"surprise\": filters.SurpriseFilter(\"resources/images/surprise.png\", \"resources/images/surprised_eye.png\"),\n",
    "}\n",
    "\n",
    "def predict_emotion(img, face_data):\n",
    "    x, y, w, h = face_data['facial_area']['x'], face_data['facial_area']['y'], face_data['facial_area']['w'], face_data['facial_area']['h']\n",
    "    cropped_face_rgb = cv2.cvtColor(img[y:y + h, x:x + w], cv2.COLOR_BGR2RGB)\n",
    "    objs = DeepFace.analyze(cropped_face_rgb, actions = ['emotion'], enforce_detection=False)\n",
    "    dominant_emotion = objs[0]['dominant_emotion']\n",
    "    cv2.putText(img, f\"Emotion: {dominant_emotion}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "    return dominant_emotion\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "ret, frame = cap.read()\n",
    "key = 0\n",
    "\n",
    "while ret and key != 27:\n",
    "    try:\n",
    "        for face in DeepFace.extract_faces(frame, detector_backend=\"yolov8\", enforce_detection=False):\n",
    "            filters_dict.get(predict_emotion(frame, face), filters.NoFilter()).apply(frame, face)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"error: {e}\")\n",
    "\n",
    "    cv2.imshow(\"Video\", frame)\n",
    "    ret, frame = cap.read()\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "# Liberar la captura y cerrar ventanas\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "id": "9cc6a544fd95b67a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Only C and default locale supported with the posix collation implementation\n",
      "Only C and default locale supported with the posix collation implementation\n",
      "Case insensitive sorting unsupported in the posix collation implementation\n",
      "Numeric mode unsupported in the posix collation implementation\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Filtro libre",
   "id": "5c06055d916b05c4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Funci贸n para guardar un gif en memoria",
   "id": "3700632e57494805"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T16:42:48.676749Z",
     "start_time": "2025-11-17T16:42:48.674512Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import filters\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import imageio\n",
    "\n",
    "def read_gif(path, width, height):\n",
    "    gif = imageio.mimread(path)\n",
    "    frames = [cv2.resize(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR), (width, height)) for frame in gif]\n",
    "    return frames"
   ],
   "id": "175f05730f4d5de7",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T16:44:52.688814Z",
     "start_time": "2025-11-17T16:44:24.467570Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mp_face_mesh = mp.solutions.face_mesh.FaceMesh(static_image_mode=True)\n",
    "\n",
    "cap = cv2.VideoCapture(\"resources/test/face_1.mp4\")\n",
    "ret, frame = cap.read()\n",
    "key = 0\n",
    "debug_filter = filters.MediapipeDebugFilter()\n",
    "\n",
    "output_gif = []\n",
    "\n",
    "frame_number = 0\n",
    "w,h,_ = frame.shape\n",
    "\n",
    "FRAME_WIDTH = 520\n",
    "FRAME_HEIGHT = int(FRAME_WIDTH * h/w)\n",
    "\n",
    "background_gif_frames = read_gif(\"resources/images/matrix_background.gif\", FRAME_WIDTH, FRAME_HEIGHT)\n",
    "while ret and key != 27:\n",
    "    try:\n",
    "        # Just for this video\n",
    "        frame = cv2.rotate(frame, cv2.ROTATE_90_CLOCKWISE)\n",
    "\n",
    "        frame = cv2.resize(frame, (FRAME_WIDTH, FRAME_HEIGHT))\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame = background_gif_frames[frame_number].copy()\n",
    "\n",
    "        for face in mp_face_mesh.process(frame_rgb).multi_face_landmarks:\n",
    "            debug_filter.apply(frame, face)\n",
    "\n",
    "        # Just for this video\n",
    "        time.sleep(0.02)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"error: {e}\")\n",
    "\n",
    "\n",
    "    cv2.imshow(\"Video\", frame)\n",
    "    output_gif.append(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
    "    ret, frame = cap.read()\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    frame_number = (frame_number + 1) % len(background_gif_frames)\n",
    "\n",
    "\n",
    "# Liberar la captura y cerrar ventanas\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "#imageio.mimsave(\"results/matrix_filter.gif\", output_gif, fps=60)"
   ],
   "id": "c7de0dea193c0e13",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1763397864.474715   19180 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1763397864.475945   22431 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 25.1.9), renderer: Mesa Intel(R) UHD Graphics (TGL GT1)\n",
      "W0000 00:00:1763397864.478891   22425 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1763397864.484139   22423 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error: 'NoneType' object is not iterable\n",
      "error: 'NoneType' object is not iterable\n",
      "error: 'NoneType' object is not iterable\n",
      "error: 'NoneType' object is not iterable\n",
      "error: 'NoneType' object is not iterable\n",
      "error: 'NoneType' object is not iterable\n",
      "error: 'NoneType' object is not iterable\n"
     ]
    }
   ],
   "execution_count": 19
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
